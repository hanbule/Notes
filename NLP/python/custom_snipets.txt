# 1.1 - (deprecated, because comma separator is buggy)
Tokenize text into tokens:
    def tokenize_1(text):
        if(text=="" or text!=text):
            return text
        else:
            tokens = re.split("(?<! т.е)(?<!\\.)\\. +(?!\\.)(?!com)|(?<![0-9]), *(?![0-9])| +", text)
            return tokens

# 1.2
Tokenize text into tokens:
    def tokenize_1(text):
        if(text=="" or text!=text):
            return text
        else:
            reg_part_1 = "(?<! т.е)(?<!\\.)\\. +(?!\\.)(?!com)"   # any dot acting like end of sent not part of token like т.е., .com, .., ...
            reg_part_2 = " *, +|(?<=[^0-9]),(?=.)|(?<=[0-9]),(?=[^0-9])|(?<=.) ,(?=.)|(?<![0-9]),(?![0-9])"   # any comma preceded/folowed by space/s except comma between digits
            reg_part_3 = " +"  # space/s
            regs = [reg_part_1, reg_part_2, reg_part_3]
            
            tokens = re.split("|".join(regs), txt)
            return tokens

Ex:
Try out 1.1 and 1.2 an compare results:
    s = "My, numbers 15 and 74,5 4, 5  4, a b , 48,67,666  c ,9"
    tokenize_1(s)  # 1.1
    tokenize_1(s)  # 1.2
    
    Correct one is ['My', 'numbers', '15', 'and', '74,5', '4', '5', '4', 'a', 'b', '48,67,666', 'c', '9']


# 2
Get list of unique tokens from text:
    def get_uniq_tokens(s):
        if(s == "" or s!=s):
            return s
        else:
            tokens = tokenize_1(s)
            tokens = list(set(tokens))
            return tokens
