# 1
Takes text and returns list of tokens:
    def tokenize_1(text):
        tokens = re.split("(?<! ั.ะต)(?<!\\.)\\. +(?!\\.)(?!com)|(?<![0-9]), *(?![0-9])| +", text)
        return tokens

# 2
Takes text and returns list of unique tokens:
    def get_uniq_tokens(s):
        if(s == "" or s!=s):
            return s
        else:
            tokens = tokenize_1(s)
            tokens = list(set(tokens))
            return tokens
