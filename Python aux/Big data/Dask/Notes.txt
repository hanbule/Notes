# Dask: 1.1.3

# 1
# Read single csv file
# Note: when dask loads file into dataframe, it partititons it into several chunks (i.e. several pandas dataframes)
        consequently each pandas dataframe has own independent indexing that starts with zero.
    
    import dask.dataframe as dd
    df = dd.read_csv("my_file.csv")
	  print("DF shape: ", len(df), len(df.columns))


# 2
# Read multiple(chunked) csv files
# Ex:
    my_file-00.csv
    my_file-01.csv
    my_file-02.csv

    import dask.dataframe as dd
    df = dd.read_csv("my_file-*.csv", blocksize=10e6)  # blocksize set to 10mb, so each partition size is 10mb. Default 64mb.
                                                       # In Dask v1.1.3, blocksize can be a number like 64000000 
						       # or a string like "64MB", "1GB"
                                                       # If set to None, each csv file will be a separate single partition.
    print("DF shape: ", len(df), len(df.columns))
    


# 3.1
# Export dask dataframe to csv:

    import dask.dataframe as dd
    df.to_csv("my_file-*.csv", index=False, columns=[...])   # by default, df is exported in chunks which is why * symbol in file name
                                                             # so that each file (df chunk) will be named as my_file-*.csv
                                                             # but with numeration instead of * symbol.
                                                             
    print(df.npartitions)  # print count of partitions to know into how many files dataframe will be splitted

3.2
# Export dask dataframe as one single file (workaround):
	ls = ddf.to_delayed()
	for idx, p in enumerate(ls):
		df = p.compute()
		print("DF shape: ", len(df), len(df.columns))

		h = False
		if(idx == 0):
			h = True
		df.to_csv('my_file.csv', mode='a', index=False, header=h)

# 4.1
# Change count of partitions

	ddf = ddf.repartition(npartitions=30)

# 4.2
# Iterate partitions (should be wo shuffling)

	for idx in range(ddf.npartitions):
		partition = ddf.get_partition(idx)             # this is just delayed object, not df
		partition = ddf.get_partition(idx).compute()   # this actual df that can be manipulated

    
# 5
# Rename columns

	print(df.columns)
	df = df.rename(columns = {col1:new_col1, col2:new_col2} )
    
    
# 6.1
# Join dataframes on their index
# Note: Joining on index works properly only when dataframes to be merged are loaded fully into memory.
        That's because by default file is loaded into multiple pandas dataframes where each one has independent indexing
        that starts with zero. So while 1st pandas index starts with zero and ends with N 
        2nd pandas index does not continue as N+1, instead it starts with zero too as all the other chunks.

	import dask.dataframe as dd
	new_df = dd.merge(df1, df2, left_index=True, right_index=True)


# 6.2
# Join dataframes on columns

	import dask.dataframe as dd
	new_df = dd.merge(df1, df2, left_on=df1_col, right_on=df2_col)   # better df1_col and df2_col be unique-value columns
    								         # or else joining will go cartesian way

# 7
# Create auto-increment id column
	
	df['id'] = 1
	df['id'] = df.id.cumsum()
	
	
# 8
# Csv into sqlite snippet (should be wo shuffling)

	import sqlite3
	
	sqlite_file = "path/to/db.sqlite"    # name of the sqlite database file
	conn = sqlite3.connect(sqlite_file)
	table_name = "..."

	for idx in range(ddf.npartitions):
		df = ddf.get_partition(idx).compute()   # get partition as pandas
		print("Partition #%d shape: (%d %d)" % (idx, len(df), len(df.columns)) )

		df.to_sql(name=table_name, con=conn, chunksize=500, if_exists='append', index=False)

# 9
# apply() func:
	ddf['new_col'] = ddf.map_partitions(lambda df : df.apply(lambda x : x['some_col'], axis=1))  # as dask df consists of dataframes

Note: the above procedure goes sequentially, i.e. one df is processed at a time, so no parallelization at all.

	
# 10.1
# Convert dask df to dask array:
	d_arr = ddf.to_dask_array()
	print(darr.shape)   # Output: (None, 25)
	print(darr.dtype)   # Output: int64
	
	d_arr = ddf.to_dask_array(lengths=True)
	print(darr.shape)   # Output: (10, 25)
	print(darr.dtype)   # Output: int64
	

# 10.2
# Convert dask array to dask df:
	import dask.dataframe as dd
	ddf = dd.from_dask_array(darr)  #  ddf will use auto-incrementing numbers as column names

# 11.1
# Store dask array in hdf5:
	import dask.array as da
	import h5py
	
	# from df to array
	darr = ddf.to_dask_array(lengths=True)

	f = h5py.File('myfile.hdf5')
	d = f.require_dataset('/data', shape=darr.shape, dtype=darr.dtype)
	da.store(darr, d)   # Reminder: myfile.hdf5 might be way larger than source csv file. Ex: 12.6gb csv -> 50.4gb hdf5
	
	f.close()
	
# 11.2
Read array from hdf5:
	import dask.array as da
	import h5py


	if __name__=="__main__":
	    f = h5py.File('myfile.hdf5')
	    d = f['/data']
	    data = da.from_array(d, chunks=(1000,1000))
	    print(d)
	    print(type(data))
	    print(data.shape)
	    
	    f.close()

# 12
# Make changes in stored dask array:
	import dask.array as da
	import h5py

	if __name__=="__main__":
	    # read array
	    f = h5py.File('myfile.hdf5')
	    d = f['/data']
	    darr = da.from_array(d, chunks=(1000,1000))
	    print(darr.shape)

	    # make changes
	    darr = darr + 1

	    # view changes of array part
	    print(darr[:30, :30].compute())    # view first 30 rows and first 30 columns

	    # save edited array back
	    da.store(darr, d)  # as the array is stored in hdf5, saving back back will be amazingly fast
	    
	    f.close()

# 13
Append new array to existing dataset (array) in hdf5:
	import dask.array as da
	import h5py

	if __name__=="__main__":
	    # open file
	    f = h5py.File('myfile.hdf5')
	    d = f['/data']
	    print('Existing shape:', d[...].shape)

	    # new array
	    new_arr  = np.random.rand(100, vec_len)
	    new_darr = da.from_array(new_arr, chunks=shape)

	    # append new array
	    d = da.concatenate([d[...], new_darr], axis=0)

	    f.close()

# 14
If you get error like:
	YAMLLoadWarning: calling yaml.load() without Loader=... is deprecated, as the default Loader is unsafe.
try reinstalling pyyaml lib to version 3.13 in python

