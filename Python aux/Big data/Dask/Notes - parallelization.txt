Note:
    To use Dask's parallelization power we:
    - always use delayed() function
    - huge file should not be stored as a whole but rather be splitted into pieces

Ex 1:
Each piece of file is read in parallel, processed in parallel and stored in parallel as soon as processing finishes:

    import dask.dataframe as dd
    from dask.delayed import delayed

    from my_custom_library import load, save

    filenames = ...
    dfs = [delayed(load)(fn) for fn in filenames]

    df = dd.from_delayed(dfs)
    df = ... # do work with dask.dataframe

    dfs = df.to_delayed()
    writes = [delayed(save)(df, fn) for df, fn in zip(dfs, filenames)]

    dd.compute(*writes)
