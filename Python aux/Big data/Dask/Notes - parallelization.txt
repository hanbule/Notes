Note:
    To use Dask's parallelization power we:
    - always use delayed() function or better Client from ditributed
    - huge file should not be stored as a whole but rather be splitted into pieces

Ex 1 - delayed():
    Each piece of file is read in parallel, processed in parallel and stored in parallel as soon as processing finishes:
    But sometimes you might not see speed gain though.

    import dask.dataframe as dd
    from dask.delayed import delayed
    # from dask import delayed, compute   # opt 2

    from my_custom_library import load, save

    filenames = ...
    dfs = [delayed(load)(fn) for fn in filenames]

    df = dd.from_delayed(dfs)
    df = ... # do work with dask.dataframe

    dfs = df.to_delayed()
    writes = [delayed(save)(df, fn) for df, fn in zip(dfs, filenames)]

    dd.compute(*writes, scheduler="processes", num_workers=4)    # opt 1, num of cores set to 4
    # compute(*writes, scheduler="processes", num_workers=4)     # opt 2


Ex 2 - Client():
    import dask.dataframe as dd
    from dask.delayed import delayed
    from dask.distributed import Client, LocalCluster
    from my_custom_library import load, save, process, summarize

    my_dict = {...}    # just some data

    lc = LocalCluster(n_workers=20, threads_per_worker=2)   # n_workers ~ no of cores will be used
    client = Client(lc)

    filenames = ...
    dfs = [client.submit(load, fn) for fn in filenames]     # line a: list of dfs loaded in parallel (lazy ops)
    dfs_p = [client.submit(process, df, deepcopy(my_data)) for df in dfs]      # line b: list of processed dfs in parallel (lazy ops)
        
    res   = client.submit(summarize, dfs_p)                 # some summarize() func that takes list of processed dfs 
                                                              as argument (lazy op)
    final_res = res.result()                                # actual execution of the whole DAG above 
                                                              result() ~ compute()

    writes = [client.submit(save, df, fn) for df, fn in zip(dfs_p, filenames)]  # some save() func that takes df and filename
                                                                                # as argument (lazy ops)
    client.gather(writes)      # actual execution of all save() funcs 
                               # gather() triggers list of lazy ops, i.e. triggers result() for every lazy op
    
    
    
# Note: if any data passed to function as argument is large (>1mb) like in above line b where we pass my_dict as argument to process(), 
# then you might encounter memory issues. We also pass df as argument, but there shouldn't be any problems as it is lazy loaded data.
# in such cases we have to ways to go:
# 1 - need to scatter data passed first, and then pass to function:
        f_my_dcit = client.scatter(my_dict, broadcast=True)   # in this case additionally set broadcast to True, 
                                                              # as my_dict is passed/mapped to many functions 
                                                              # i.e. it's like single my_dict is shared by many functions
        dfs_p = [client.submit(process, df, f_my_dict) for df in dfs]
# 2 - pass data to function by reading/loading it within function as opposed to passing it as arg:
        def process():
            my_dict = json.load(...)

# 2nd way might be better as in 1st way the whole data is passed from client, which requires fitting the whole thing into local memeory,
# copying that data and serialization costs along the way.
