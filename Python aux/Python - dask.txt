# 1
# read single csv file
# Note: when dask loads file into dataframe, it partititons it into several chunks (i.e. several pandas dataframes)
        consequently each pandas dataframe has own independent indexing that starts with zero.
    
    import dask.dataframe as dd
    df = dd.read_csv("my_file.csv")
	  print("DF shape: ", len(df), len(df.columns))


# 2
# read multiple(chunked) csv files
# Ex:
    my_file-00.csv
    my_file-01.csv
    my_file-02.csv

    import dask.dataframe as dd
    df = dd.read_csv("my_file-*.csv")
	  print("DF shape: ", len(df), len(df.columns))
    


# 3
# export dataframe to csv

    import dask.dataframe as dd
    df.to_csv("my_file-*.csv", index=False, columns=[...])   # by default, df is exported in chunks which is why * symbol in file name
                                                             # so that each file (df chunk) will be named as my_file-*.csv
                                                             # but with numeration instead of * symbol.
                                                             
    print(df.npartitions)  # print count of partitions to know into how many files dataframe will be splitted


# 4.1
# Change count of partitions

	ddf = ddf.repartition(npartitions=30)

# 4.2
# Iterate partitions (should be wo shuffling)

	for idx in range(ddf.npartitions):
		partition = ddf.get_partition(idx)             # this is just delayed object, not df
		partition = ddf.get_partition(idx).compute()   # this actual df that can be manipulated

    
# 5
# Rename columns

	print(df.columns)
	df = df.rename(columns = {col1:new_col1, col2:new_col2} )
    
    
# 6.1
# Join dataframes on their index
# Note: Joining on index works properly only when dataframes to be merged are loaded fully into memory.
        That's because by default file is loaded into multiple pandas dataframes where each one has independent indexing
        that starts with zero. So while 1st pandas index starts with zero and ends with N 
        2nd pandas index does not continue as N+1, instead it starts with zero too as all the other chunks.

	import dask.dataframe as dd
	new_df = dd.merge(df1, df2, left_index=True, right_index=True)


# 6.2
# Join dataframes on columns

	import dask.dataframe as dd
	new_df = dd.merge(df1, df2, left_on=df1_col, right_on=df2_col)   # better df1_col and df2_col be unique-value columns
    								         # or else joining will go cartesian way

# 7
# Create auto-increment id column
	
	df['id'] = 1
	df['id'] = df.id.cumsum()
	
	
# 8
# csv into sqlite snippet (should be wo shuffling)

	import sqlite3
	
	sqlite_file = "path/to/db.sqlite"    # name of the sqlite database file
	conn = sqlite3.connect(sqlite_file)
	table_name = "..."

	for idx in range(ddf.npartitions):
		df = ddf.get_partition(idx).compute()   # get partition as pandas
		print("Partition #%d shape: (%d %d)" % (idx, len(df), len(df.columns)) )

		df.to_sql(name=table_name, con=conn, chunksize=500, if_exists='append', index=False)
