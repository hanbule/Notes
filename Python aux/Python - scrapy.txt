# 1
# Fix unclosed tags

    from scrapy.selector import Selector
    from BeautifulSoup import BeautifulSoup
    
    soup = BeautifulSoup(response.body)
    selector_1 = Selector(text=soup.prettify())
    
and now you can extract from selector_1 where all unclosed tags are fixed
    
    selector_1.xpath("...").extract()


# 2
# Html code to selector
    Selector(text=html_code)


# Get body from selector
    body = selector.select("//body").extract()


# Get divs from body of selector
    body_ls = selector.select("//body/div").extract()  # select method might be deprecated
    body_ls = selector.xpath("//body/div").extract()   # so try to use xpath instead


# 3
# / and // in xpath

short explanation:
tag_1/tag_2 tells that tag_2 should be right next to tag_1.
tag_1//tag_2 tells that tag_2 should be within the tag_1 and not necessarily be right next to tag_2.


Examples:
    <body>
        <tag_a>
            <tag_b>
                <tag_d class='A'>
                </tag_d>
            </tag_b>
            <tag_c>
                <tag_d class='A'>
                </tag_d>
            </tag_c>
            <tag_c>
                <tag_d class='B'>
                </tag_d>
            </tag_c>
        </tag_a>
    </body>

    Ex 1:
    To get everything from <body> tag:
        response.xpath("//body").extract()

    Ex 2:
    To get everything from <tag> tag:
        response.xpath("//body/tag_a").extract()
        or
        response.xpath("//tag_a").extract()

    Ex 3:
    To get <tag_b>:
        response.xpath("//body/tag_a/tag_b").extract()
        or
        response.xpath("//body//tag_b").extract()
        or
        response.xpath("//tag_a/tag_b").extract()
        or
        response.xpath("//tag_b").extract()

    Ex 4.1:
    To get all <tag_c> tags:
        response.xpath("//body/tag_a/tag_c").extract()
        or
        response.xpath("//body//tag_c").extract()
        or
        response.xpath("//tag_c").extract()

     Ex 4.2:
     To get 1st or secor=nd <tag_c> tag:
        response.xpath("//body/tag_a/tag_c[1]").extract()
        response.xpath("//body/tag_a/tag_c[2]").extract()
        or
        response.xpath("//body//tag_c[1]").extract()
        response.xpath("//body//tag_c[2]").extract()
        or
        response.xpath("//tag_c[1]").extract()
        response.xpath("//tag_c[2]").extract()


# 4.1
To scrape data from page, write scrapy spider code, save in .py file and run it in terminal:
    scrapy runspider my_spider_file.py


# 4.2
Scrapy spider snippet:
'''
Python: 2.7
Scrapy: 1.6.0
'''

# -*- coding: utf-8 -*-
import sys
import locale
reload(sys)
sys.setdefaultencoding('utf-8')

import scrapy
from scrapy.http import Request, HtmlResponse
from scrapy.selector import Selector
from scrapy.spiders import CrawlSpider, Rule
from scrapy.linkextractors import LinkExtractor
from BeautifulSoup import BeautifulSoup
import json
from collections import defaultdict, OrderedDict


# must-define params
heading = "football"
page_cnt = 5

# global article meta-container
article = OrderedDict()

class NewsSpider(scrapy.Spider):
    name = 'my_news'    
    allowed_domains = ['sports.kz']
    start_urls = ["http://sports.kz/"+heading+"?page=%d" % page for page in range(1, (page_cnt+1))]  # pages, each with list of news

    # method applied by spider to each of webpages mentioned in start_urls
    def parse(self, response):
        # get article urls from page
        for a_tag in response.xpath("//ul[@class='news_list_']/li/a[2]").extract():
            article_url = Selector(text=a_tag).xpath("//a/@href").extract()

            # just for testing purposes
            with open("article_links.txt", "a") as outfile:
                outfile.write("https://www.sports.kz" + article_url[0])
                outfile.write("\n")

            # then follow (open) each article url and apply parse_article() to that article page
            yield response.follow("https://www.sports.kz" + article_url[0], callback=self.parse_article)

    # dummy for testing
    def parse_article_d(self, response):
        pass

    # my callback function that scrapes article details from article page
    def parse_article(self, response):
        # 1
        # scrape data


        # 1.1
        article['topic']          = heading
        # 1.2
        tags_ls                   = response.xpath("//div[@class='c_left']//div[@class='news_read_bot']/ul/li/a/descendant::text()").extract()
        article['tags']           = ",".join(tags_ls) if len(tags_ls)!=0 else ""
        # 1.3
        date_pub                  = response.xpath("//div[@class='c_left']//div[@class='news_main_ under_']//span[@class='datetype']/u/descendant::text()").extract()
        article['date_pub']       = date_pub[0] if len(date_pub)!=0 else ""
        # 1.4
        # views                     = response.xpath("//div[@class='leftside']//div[@class='news-single']//div[@class='info']/div[@class='v']/descendant::text()").extract()
        # article['views']          = views[0] if len(views)!=0 else ""
        # 1.5
        comments_count            = response.xpath("//div[@class='c_left']//div[@class='news_main_ under_']//div[@class='news_read_txt']/ul/a/li/span/text()").extract()
        article['comments_count'] = comments_count[0] if len(comments_count)!=0 else ""


        # 1.6
        article['url']      = response.url
        # 1.7
        author              = response.xpath("//div[@class='c_left']//div[@class='news_main_ under_']//div[@class='copyavtor']/dt/div/span/a/descendant::text()").extract()
        article['author']   = author[0] if len(author)!=0 else ""
        # 1.8
        title               = response.xpath("//div[@class='c_left']//div[@class='news_main_ under_']/h2/descendant::text()").extract()
        article['title']    = " ".join(title) if len(title)!=0 else ""
        
        # 1.9
        article['text']     = ""

        text_parts = response.xpath("//div[@class='c_left']//div[@class='news_main_ under_']//div[@class='news_read_txt']/p")        
        for t in text_parts:
            # print t
            if(len(t.xpath('text()').extract()) != 0):
                article['text'] = article['text'] + " " + " ".join([e for e in t.xpath('descendant::text()').extract()])

        # 2
        # persist article details
        with open("rk_news_"+heading+".json", 'ab') as f1:
            json.dump(article, f1, ensure_ascii=False, indent=None)
            f1.write(',\n')

        # uncomment if you like to persist outputs using -o command in terminal
        # yield article



